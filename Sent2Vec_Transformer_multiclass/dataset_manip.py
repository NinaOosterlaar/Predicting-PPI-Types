import pandas as pd
from collections import defaultdict
import pickle
import numpy as np
import random
import torch
import torch.utils.data as data
import gzip
import json
from obtain_data import retrieve_interactions, create_gene_GO_dict, delete_duplicates, retrieve_organism_interactions



def go_embeddings_to_dict(go_embed_pth):
    
    """
    read the embeddings generated by Node2vec
    :return dict of GOid -> embeddings
    """
    with open(go_embed_pth, 'r') as f:
        embeddings = json.load(f)
    return embeddings
    # #load the embeddings generated by node2vec for each index GO, ignore first information line
    # embeddings_dict = {}
    # embeddings = open(go_embed_pth).read().splitlines()[1:]
    # embeddings = [ x.split(" ") for x in embeddings ]

    # for i in range(0, len(embeddings)):
    #     #set the GO id as the key
    #     key = int(embeddings[i][0]) 
    #     #add all the dimension of the embedings as a list of floats
    #     embeddings_dict[key] = [ float(x) for x in embeddings[i][1:]]
        
    # return embeddings_dict


def get_max_len_seq(dataset):
    """Finds the protein with the most annotations and returns the size"""
    batch_features, batch_labels, batch_ids  = zip(*dataset)
    batch_features = np.array(batch_features, dtype='object')
    
    max_len = 0
    for i in range(0, batch_features.shape[0]):
        max_len = max(max_len, len(batch_features[i][0]), len(batch_features[i][1]))
    return max_len  



"""This part of the code allows the model to generate embeddings on the go when there is a new batch generated.
This is way more memory efficient than emmbeding the entire dataset and then keep it in memory.
"""
class Dataset_stringDB(torch.utils.data.Dataset):
    #Characterizes a dataset for PyTorch
    def __init__(self, all_ppi, labels, protein_go_anno,  go_embed_dict, shuffle):
        self.all_ppi = all_ppi
        self.labels = labels        
        self.protein_go_anno = protein_go_anno
        self.go_emb_dict = go_embed_dict
        self.shuffle = shuffle
    def __len__(self):
        return len(self.all_ppi)

    def __getitem__(self, index):

        label = self.labels[index]    
        protA,protB = self.all_ppi[index]
        features, idi =  get_embedded_proteins(self.protein_go_anno, self.go_emb_dict.keys(), self.go_emb_dict, self.shuffle, protA, protB)
        return np.array((features, label, idi), dtype=object)
    
    

def filter_interactions(ppi, protein_go_anno, go_embed_dict, intr_set_size_filter, max_intr_size):
    
    """ Checks the annotations of the interacting proteins for certain filters 
    Args:
        ppi (list): the protein interactions ex. ['ProtA ProtB', ...]
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict_pth (str): path to the dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_pth (str): path to the embedings generated by node2vec for GO terms
        aliases_path (str): path to the protein aliases 
        stringDB (bool): there are two different functions that generate dictionaries for protein annotations depending on the dataset
        go_filter (str): what type of GO temrs to keep: ALL, CC, BP or MF        
        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10
        max_intr_size (int): the maximum number of interactions to add to the dataset 
        
    Returns:
    list: returns only the valid interactions
    """ 

    # #load the mapping from 'GO name' to index ex: GO0001 -> 1
    # with open(go_id_dict_pth, 'rb') as fp:
    #     go_id_dict = pickle.load(fp)
        
    # #load the mapping from 'GO name' to namespace: GO0001 -> 'biological_process'
    # with open(go_name_space_dict_pth, 'rb') as fp:
    #     go_name_space_dict = pickle.load(fp)
    
    filtered_ppi = []
    rejected_no_annot = 0
    rejected_filter = 0
    for (protA, protB) in ppi:
        
        for prot in [protA, protB]:
            #fileter those GO terms that are not found in the GO file used by us
            protein_go_anno[prot] = [go for go in protein_go_anno[prot] if go in go_embed_dict.keys()]
        
        #check if both proteins have atleast 1 GO term associated
        if len(protein_go_anno[protA]) >0 and len(protein_go_anno[protB]) >0:
            
            # check if intr size is in range defined by filter
            intr_set_size = (len(protein_go_anno[protA]) + len(protein_go_anno[protB]))
            if intr_set_size >= intr_set_size_filter[0] and intr_set_size <= intr_set_size_filter[1]:
                
                #break if we added enoguh interactions to the dataset
                if max_intr_size == len(filtered_ppi):
                    return filtered_ppi, protein_go_anno
                filtered_ppi.append((protA, protB))
            else:
                rejected_filter += 1
        else:
            rejected_no_annot += 1
         
    print("Rejected interactions where at least one protein has no annotation: ", rejected_no_annot)
    print("Number of interactions:", len(filtered_ppi))
    return filtered_ppi, protein_go_anno    
    
    
def get_embedded_proteins(protein_go_anno, go_id_dict, go_emb_dict, shuffle, protA, protB):
    
    
    """ Embedding only 2 proteins
    Args:
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict (dict): dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_dict (dict): embedings dict generated by node2vec for GO terms
        shuffle (function): shuffle function for the GO term list, default is no shuffle
        protA (string): protA name
        protB (string): protB name
        
    Returns:
    tuple: (protein embeddings, protein label information)
    """ 

    if protA in protein_go_anno and protB in protein_go_anno:
        protein_go_anno[protA] = [go for go in protein_go_anno[protA] if go in go_id_dict]
        protein_go_anno[protB] = [go for go in protein_go_anno[protB] if go in go_id_dict]
    
    emb_protA = [ go for go in protein_go_anno[protA] ]
    emb_protB = [ go for go in protein_go_anno[protB] ]
        
    #shuffle if requierd for experiments
    if shuffle is not None:
        
        #zip embeddings and GO labels to KEEP THE SAME MAPPING!!!!
        shuffledA = list(zip(emb_protA, protein_go_anno[protA]))
        shuffledB = list(zip(emb_protB, protein_go_anno[protB]))
        
        shuffle(shuffledA)
        shuffle(shuffledB)
        
        emb_protA, protein_go_anno[protA] = zip(*shuffledA)
        emb_protB, protein_go_anno[protB] = zip(*shuffledB)
    
    emb_protA = [ go_emb_dict[go] for go in emb_protA]
    emb_protB = [ go_emb_dict[go] for go in emb_protB]
    
    # Shape ( [protALen, node2vecDim], [protBLen, node2vecDim] )
    # Shape ( [1, protALen], [1, protBLen] )
    return (np.array(emb_protA), np.array(emb_protB)), ((protA, protein_go_anno[protA]), (protB, protein_go_anno[protB]))





def get_dataset_split_stringDB(interactions, go_embed_pth, shuffle, ratio = [0.8, 0.2, 0], organism = [0], intr_set_size_filter = [0,500], max_intr_size = None, no_actions = True):
    
    """ Splitting up the interaction data into train/valid/test and generating embeddings 
    Args:
        interactions (list): list of tuples containing interaction_type and confidence_score
        go_id_dict_pth (str): path to the the GO id dict, e.g. GO1 -> 1
        shuffle (funct): function for shuffling the GO terms
        aliases_path (str): path to the aliases file 
        ratio (list): list of 3 floats specifing the split between train/valid/test
        stringDB (bool): weather we are using stringDB datasets or the Jains datasets
        go_name_space_dict_pth (str): path to the dictionary between name of GO terms and explanation e.g. GO1 -> 'MF molecular function'
        go_filter (str): filter for the GO terms (what terms to keep), e.g.  ALL, CC, BP, MF
        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10
        max_intr_size (int): the maximum number of interactions to add to the dataset 
        
    Returns:
    Dataset_stringDB: 4 datasets objects for the train, valid, test and full datasets
    """ 
    go_embed_dict = go_embeddings_to_dict(go_embed_pth)
    
    all_ppi_list = []
    save_location = []
    for c, interaction in enumerate(interactions):
        if interaction[0] == 1:
            save_location.append(c)
        if interaction[0] == 2:
            save_location.append(c)
    if organism[0] == 0 and len(organism) == 1:
        for c, interaction in enumerate(interactions):
            all_ppi_list.append(retrieve_interactions(interaction[0], interaction[1]))
    else:
        for c, interaction in enumerate(interactions):
            lst = []
            for orga in organism:
                lst += retrieve_organism_interactions(interaction[0], interaction[1], orga)
            all_ppi_list.append(lst)
    
    all_ppi_list[save_location[0]], all_ppi_list[save_location[1]] = delete_duplicates(all_ppi_list[save_location[0]], all_ppi_list[save_location[1]])
    
    for all_ppi in all_ppi_list:
        print(len(all_ppi))
    
    
    for i in range(0, len(all_ppi_list)):
        all_ppi_list[i] = list(set(all_ppi_list[i]))
    
    if no_actions:
        length = (len(all_ppi_list[0]) + len(all_ppi_list[1])) // 2  
        all_ppi_list[2] = random.sample(set(all_ppi_list[2]), length)   
    
     
    protein_go_anno = create_gene_GO_dict()
    save_proteins = []
    for ppi_list in all_ppi_list:
        for ppi in ppi_list:
            for p in ppi:
                if p not in protein_go_anno:
                    save_proteins.append(ppi)
    for ppi in save_proteins:
        for ppi_list in all_ppi_list:
            if ppi in ppi_list:
                ppi_list.remove(ppi)
    
    for c, ppi in enumerate(all_ppi_list):
        all_ppi_list[c], protein_go_anno = filter_interactions(ppi, protein_go_anno, go_embed_dict, intr_set_size_filter, max_intr_size)
        
    labels = []
    for c, ppi in enumerate(all_ppi_list):
        labels += [c] * len(ppi)
    all_ppi = []
    for ppi in all_ppi_list:
        all_ppi += ppi
    ##shuffle the data such that the poz and neg don't appear toghether
    full_dataset = list(zip(all_ppi, labels))
    random.shuffle(full_dataset)
    all_ppi, labels = zip(*full_dataset)
    full_dataset = Dataset_stringDB(all_ppi, labels, protein_go_anno, go_embed_dict, shuffle)

    sz = len(full_dataset)
    train, valid, test = data.random_split(full_dataset, [int(ratio[0]*sz), int(ratio[1]*sz) , sz - (int(ratio[0]*sz) + int(ratio[1]*sz)) ] )

    return train, valid, test, full_dataset



