import pickle
import numpy as np
import random
from sklearn.model_selection import train_test_split
from obtain_data import retrieve_interactions, create_gene_GO_dict, delete_duplicates, retrieve_organism_interactions



def go_embeddings_to_dict(go_embed_pth):
    
    """
    read the embeddings generated by Node2vec
    :return dict of GOid -> embeddings
    """
    #load the embeddings generated by node2vec for each index GO, ignore first information line
    embeddings_dict = {}
    embeddings = open(go_embed_pth).read().splitlines()[1:]
    embeddings = [ x.split(" ") for x in embeddings ]

    for i in range(0, len(embeddings)):
        #set the GO id as the key
        key = int(embeddings[i][0]) 
        #add all the dimension of the embedings as a list of floats
        embeddings_dict[key] = [ float(x) for x in embeddings[i][1:]]
        
    return embeddings_dict

    

def filter_interactions(ppi, protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size):
    
    """ Checks the annotations of the interacting proteins for certain filters 
    Args:
        ppi (list): the protein interactions ex. ['ProtA ProtB', ...]
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict_pth (str): path to the dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_pth (str): path to the embedings generated by node2vec for GO terms
        aliases_path (str): path to the protein aliases 
        stringDB (bool): there are two different functions that generate dictionaries for protein annotations depending on the dataset
        go_filter (str): what type of GO temrs to keep: ALL, CC, BP or MF        
        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10
        max_intr_size (int): the maximum number of interactions to add to the dataset 
        
    Returns:
    list: returns only the valid interactions
    """ 

    #load the mapping from 'GO name' to index ex: GO0001 -> 1
    with open(go_id_dict_pth, 'rb') as fp:
        go_id_dict = pickle.load(fp)
        
    #load the mapping from 'GO name' to namespace: GO0001 -> 'biological_process'
    with open(go_name_space_dict_pth, 'rb') as fp:
        go_name_space_dict = pickle.load(fp)
    
    filtered_ppi = []
    rejected_no_annot = 0
    rejected_filter = 0
    for (protA, protB) in ppi:
        
        for prot in [protA, protB]:
            #fileter those GO terms that are not found in the GO file used by us
            protein_go_anno[prot] = [go for go in protein_go_anno[prot] if go in go_id_dict]

            #Filter GO terms according to Ontology terms ('cellular', 'biological', 'moLecular')
            if go_filter != "ALL":
                protein_go_anno[prot] = [go for go in protein_go_anno[prot] if go_filter in go_name_space_dict[go]]
        
        #check if both proteins have atleast 1 GO term associated
        if len(protein_go_anno[protA]) >0 and len(protein_go_anno[protB]) >0:
            
            # check if intr size is in range defined by filter
            intr_set_size = (len(protein_go_anno[protA]) + len(protein_go_anno[protB]))
            if intr_set_size >= intr_set_size_filter[0] and intr_set_size <= intr_set_size_filter[1]:
                
                #break if we added enoguh interactions to the dataset
                if max_intr_size == len(filtered_ppi):
                    return filtered_ppi, protein_go_anno
                filtered_ppi.append((protA, protB))
            else:
                rejected_filter += 1
        else:
            rejected_no_annot += 1
         
    print("Rejected interactions where at least one protein has no annotation: ", rejected_no_annot)
    print(f"Rejected interactions where go_filter={go_filter} and intr_set_size_filter={intr_set_size_filter}: ", rejected_filter)
    print("Number of interactions:", len(filtered_ppi))
    return filtered_ppi, protein_go_anno    
    
def get_embedded_proteins(protein_go_anno, go_id_dict, go_emb_dict, shuffle, protA, protB):
    
    
    """ Embedding only 2 proteins
    Args:
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict (dict): dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_dict (dict): embedings dict generated by node2vec for GO terms
        shuffle (function): shuffle function for the GO term list, default is no shuffle
        protA (string): protA name
        protB (string): protB name
        
    Returns:
    tuple: (protein embeddings, protein label information)
    """ 
    
   
    """ Embedding only 2 proteins
    Args:
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict (dict): dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_dict (dict): embedings dict generated by node2vec for GO terms
        protA (string): protA name
        protB (string): protB name
        
    Returns:
    tuple: (protein embeddings, protein label information)
    """ 
    protein_go_anno[protA] = [go for go in protein_go_anno[protA] if go in go_id_dict]
    protein_go_anno[protB] = [go for go in protein_go_anno[protB] if go in go_id_dict]
    
    # print(protein_go_anno[protA])
    
    emb_protA = [ go_id_dict[go] for go in protein_go_anno[protA] ]
    emb_protB = [ go_id_dict[go] for go in protein_go_anno[protB] ]
    
        
    #shuffle if requierd for experiments
    if shuffle is not None:
        
        #zip embeddings and GO labels to KEEP THE SAME MAPPING!!!!
        shuffledA = list(zip(emb_protA, protein_go_anno[protA]))
        shuffledB = list(zip(emb_protB, protein_go_anno[protB]))
        
               
        shuffle(shuffledA)
        shuffle(shuffledB)
        
        emb_protA, protein_go_anno[protA] = zip(*shuffledA)
        emb_protB, protein_go_anno[protB] = zip(*shuffledB)
        
    
    emb_protA = [ go_emb_dict[go] for go in emb_protA]
    emb_protB = [ go_emb_dict[go] for go in emb_protB]
    
    # Shape ( [protALen, node2vecDim], [protBLen, node2vecDim] )
    # Shape ( [1, protALen], [1, protBLen] )
    return np.concatenate((np.array(emb_protA), np.array(emb_protB)), axis = 0), ((protA, protein_go_anno[protA]), (protB, protein_go_anno[protB]))



def get_dataset_split_stringDB(confidence_score_pos, confidence_score_neg, go_id_dict_pth, go_embed_pth, shuffle, aliases_path = "", test_size = 0.2,\
                               stringDB = True, organism = 0, go_name_space_dict_pth = "datasets\go_namespace_dict", go_filter = "ALL", intr_set_size_filter = [0,500], max_intr_size = None):
    
    """ Splitting up the interaction data into train/valid/test and generating embeddings 
    Args:
        confidence_score (float): the confidence score for the PPI
        go_id_dict_pth (str): path to the the GO id dict, e.g. GO1 -> 1
        shuffle (funct): function for shuffling the GO terms
        aliases_path (str): path to the aliases file 
        ratio (list): list of 3 floats specifing the split between train/valid/test
        stringDB (bool): weather we are using stringDB datasets or the Jains datasets
        go_name_space_dict_pth (str): path to the dictionary between name of GO terms and explanation e.g. GO1 -> 'MF molecular function'
        go_filter (str): filter for the GO terms (what terms to keep), e.g.  ALL, CC, BP, MF
        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10
        max_intr_size (int): the maximum number of interactions to add to the dataset 
        
    Returns:
    Dataset_stringDB: 4 datasets objects for the train, valid, test and full datasets
    """ 
    if organism == 0:
        ppi_poz = retrieve_interactions(1, confidence_score_pos)
        ppi_neg = retrieve_interactions(2, confidence_score_neg)
    else:
        print('reached')
        ppi_poz = retrieve_organism_interactions(1, confidence_score_pos, organism)
        ppi_neg = retrieve_organism_interactions(2, confidence_score_neg, organism)
        
    ppi_poz, ppi_neg = delete_duplicates(ppi_poz, ppi_neg) 
    ppi_poz = list(set(ppi_poz))
    ppi_neg = list(set(ppi_neg))
     
    protein_go_anno = create_gene_GO_dict()
    
    ppi_poz, updated_protein_go_anno = filter_interactions(ppi_poz, protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)
    ppi_neg, updated_protein_go_anno = filter_interactions(ppi_neg, updated_protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)
    
    all_ppi = ppi_poz + ppi_neg
    labels =  [1] * len(ppi_poz) + [0] * len(ppi_neg)
    
    # Open the relevant GO data
    with open(go_id_dict_pth, 'rb') as fp:
            go_id_dict = pickle.load(fp)
    go_emb_dict = go_embeddings_to_dict(go_embed_pth)
    
    ##shuffle the data such that the poz and neg don't appear toghether
    full_dataset = list(zip(all_ppi, labels))
    random.shuffle(full_dataset)
    all_ppi, labels = zip(*full_dataset)
    # full_dataset = Dataset_stringDB(all_ppi, labels, updated_protein_go_anno, go_id_dict_pth, go_embed_pth, shuffle, aliases_path, stringDB)
    sz = len(full_dataset)
    embeddings = []
    
    for ppi in all_ppi:
        embedding, proteins = get_embedded_proteins(updated_protein_go_anno, go_id_dict, go_emb_dict, shuffle, ppi[0], ppi[1])
        embeddings.append(embedding)
    
    train_embeddings, test_embeddings, train_labels, test_labels = train_test_split(embeddings, labels, test_size=test_size)
     
    return np.array(train_embeddings), np.array(test_embeddings), np.array(train_labels), np.array(test_labels)



    
