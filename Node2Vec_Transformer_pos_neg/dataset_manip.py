import pickle
import numpy as np
import random
import torch
import torch.utils.data as data
import gzip
import json
from obtain_data import retrieve_interactions, create_gene_GO_dict, delete_duplicates, retrieve_organism_interactions



def go_embeddings_to_dict(go_embed_pth):
    
    """
    read the embeddings generated by Node2vec
    :return dict of GOid -> embeddings
    """
    #load the embeddings generated by node2vec for each index GO, ignore first information line
    embeddings_dict = {}
    embeddings = open(go_embed_pth).read().splitlines()[1:]
    embeddings = [ x.split(" ") for x in embeddings ]

    for i in range(0, len(embeddings)):
        #set the GO id as the key
        key = int(embeddings[i][0]) 
        #add all the dimension of the embedings as a list of floats
        embeddings_dict[key] = [ float(x) for x in embeddings[i][1:]]
        
    return embeddings_dict


def get_max_len_seq(dataset):
    """Finds the protein with the most annotations and returns the size"""
    batch_features, batch_labels, batch_ids  = zip(*dataset)
    batch_features = np.array(batch_features, dtype='object')
    
    max_len = 0
    for i in range(0, batch_features.shape[0]):
        max_len = max(max_len, len(batch_features[i][0]), len(batch_features[i][1]))
    return max_len  



"""This part of the code allows the model to generate embeddings on the go when there is a new batch generated.
This is way more memory efficient than emmbeding the entire dataset and then keep it in memory.
"""
class Dataset_stringDB(torch.utils.data.Dataset):
    #Characterizes a dataset for PyTorch
    def __init__(self, all_ppi, labels, protein_go_anno, go_id_dict_pth, go_embed_pth,  shuffle, aliases_path, stringDB):
        self.all_ppi = all_ppi
        self.labels = labels
        
        #load the mapping from 'GO name' to index ex: GO0001 to 1
        with open(go_id_dict_pth, 'rb') as fp:
            go_id_dict = pickle.load(fp)
        go_emb_dict = go_embeddings_to_dict(go_embed_pth)
        
        self.protein_go_anno = protein_go_anno
        self.go_id_dict = go_id_dict
        self.go_emb_dict = go_emb_dict
        self.stringDB = stringDB
        self.shuffle = shuffle
    def __len__(self):
        return len(self.all_ppi)

    def __getitem__(self, index):

        label = self.labels[index]    
        protA,protB = self.all_ppi[index]
        features, idi =  get_embedded_proteins(self.protein_go_anno, self.go_id_dict, self.go_emb_dict, self.shuffle, protA, protB)
        return np.array((features, label, idi), dtype=object)
    
    

def filter_interactions(ppi, protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size):
    
    """ Checks the annotations of the interacting proteins for certain filters 
    Args:
        ppi (list): the protein interactions ex. ['ProtA ProtB', ...]
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict_pth (str): path to the dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_pth (str): path to the embedings generated by node2vec for GO terms
        aliases_path (str): path to the protein aliases 
        stringDB (bool): there are two different functions that generate dictionaries for protein annotations depending on the dataset
        go_filter (str): what type of GO temrs to keep: ALL, CC, BP or MF        
        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10
        max_intr_size (int): the maximum number of interactions to add to the dataset 
        
    Returns:
    list: returns only the valid interactions
    """ 

    #load the mapping from 'GO name' to index ex: GO0001 -> 1
    with open(go_id_dict_pth, 'rb') as fp:
        go_id_dict = pickle.load(fp)
        
    #load the mapping from 'GO name' to namespace: GO0001 -> 'biological_process'
    with open(go_name_space_dict_pth, 'rb') as fp:
        go_name_space_dict = pickle.load(fp)
    
    filtered_ppi = []
    rejected_no_annot = 0
    rejected_filter = 0
    for (protA, protB) in ppi:
        
        for prot in [protA, protB]:
            #fileter those GO terms that are not found in the GO file used by us
            protein_go_anno[prot] = [go for go in protein_go_anno[prot] if go in go_id_dict]

            #Filter GO terms according to Ontology terms ('cellular', 'biological', 'moLecular')
            if go_filter != "ALL":
                protein_go_anno[prot] = [go for go in protein_go_anno[prot] if go_filter in go_name_space_dict[go]]
        
        #check if both proteins have atleast 1 GO term associated
        if len(protein_go_anno[protA]) >0 and len(protein_go_anno[protB]) >0:
            
            # check if intr size is in range defined by filter
            intr_set_size = (len(protein_go_anno[protA]) + len(protein_go_anno[protB]))
            if intr_set_size >= intr_set_size_filter[0] and intr_set_size <= intr_set_size_filter[1]:
                
                #break if we added enoguh interactions to the dataset
                if max_intr_size == len(filtered_ppi):
                    return filtered_ppi, protein_go_anno
                filtered_ppi.append((protA, protB))
            else:
                rejected_filter += 1
        else:
            rejected_no_annot += 1
         
    print("Rejected interactions where at least one protein has no annotation: ", rejected_no_annot)
    print(f"Rejected interactions where go_filter={go_filter} and intr_set_size_filter={intr_set_size_filter}: ", rejected_filter)
    print("Number of interactions:", len(filtered_ppi))
    return filtered_ppi, protein_go_anno    
    
def get_embedded_proteins(protein_go_anno, go_id_dict, go_emb_dict, shuffle, protA, protB):
    
    
    """ Embedding only 2 proteins
    Args:
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict (dict): dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_dict (dict): embedings dict generated by node2vec for GO terms
        shuffle (function): shuffle function for the GO term list, default is no shuffle
        protA (string): protA name
        protB (string): protB name
        
    Returns:
    tuple: (protein embeddings, protein label information)
    """ 
    
   
    """ Embedding only 2 proteins
    Args:
        protein_go_anno_pth (str): path to the annotation file
        go_id_dict (dict): dictionary of 'GOname -> 1', 1 representing the index found in node2vec edgelist
        go_embed_dict (dict): embedings dict generated by node2vec for GO terms
        protA (string): protA name
        protB (string): protB name
        
    Returns:
    tuple: (protein embeddings, protein label information)
    """ 
    
    protein_go_anno[protA] = [go for go in protein_go_anno[protA] if go in go_id_dict]
    protein_go_anno[protB] = [go for go in protein_go_anno[protB] if go in go_id_dict]
    
    emb_protA = [ go_id_dict[go] for go in protein_go_anno[protA] ]
    emb_protB = [ go_id_dict[go] for go in protein_go_anno[protB] ]
        
    #shuffle if requierd for experiments
    if shuffle is not None:
        
        #zip embeddings and GO labels to KEEP THE SAME MAPPING!!!!
        shuffledA = list(zip(emb_protA, protein_go_anno[protA]))
        shuffledB = list(zip(emb_protB, protein_go_anno[protB]))
        
        shuffle(shuffledA)
        shuffle(shuffledB)
        
        emb_protA, protein_go_anno[protA] = zip(*shuffledA)
        emb_protB, protein_go_anno[protB] = zip(*shuffledB)
    
    emb_protA = [ go_emb_dict[go] for go in emb_protA]
    emb_protB = [ go_emb_dict[go] for go in emb_protB]
    
    # Shape ( [protALen, node2vecDim], [protBLen, node2vecDim] )
    # Shape ( [1, protALen], [1, protBLen] )
    return (np.array(emb_protA), np.array(emb_protB)), ((protA, protein_go_anno[protA]), (protB, protein_go_anno[protB]))



def get_dataset_split_stringDB(confidence_score_pos, confidence_score_neg, go_id_dict_pth, go_embed_pth, shuffle, aliases_path = "", ratio = [0.8, 0.2, 0],\
                               stringDB = True, organism = [0], go_name_space_dict_pth = "datasets/transformerGO-dataset/go-terms/go_namespace_dict", go_filter = "ALL", intr_set_size_filter = [0,500], max_intr_size = None):
    
    """ Splitting up the interaction data into train/valid/test and generating embeddings 
    Args:
        confidence_score (float): the confidence score for the PPI
        go_id_dict_pth (str): path to the the GO id dict, e.g. GO1 -> 1
        shuffle (funct): function for shuffling the GO terms
        aliases_path (str): path to the aliases file 
        ratio (list): list of 3 floats specifing the split between train/valid/test
        stringDB (bool): weather we are using stringDB datasets or the Jains datasets
        go_name_space_dict_pth (str): path to the dictionary between name of GO terms and explanation e.g. GO1 -> 'MF molecular function'
        go_filter (str): filter for the GO terms (what terms to keep), e.g.  ALL, CC, BP, MF
        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10
        max_intr_size (int): the maximum number of interactions to add to the dataset 
        
    Returns:
    Dataset_stringDB: 4 datasets objects for the train, valid, test and full datasets
    """ 
    ppi_poz = []
    ppi_neg = []
    
    if organism[0] == 0 and len(organism) == 1:
        ppi_poz = retrieve_interactions(1, confidence_score_pos)
        ppi_neg = retrieve_interactions(2, confidence_score_neg)
    else:
        for orga in organism:
            print('reached')
            ppi_poz += retrieve_organism_interactions(1, confidence_score_pos, orga)
            ppi_neg += retrieve_organism_interactions(2, confidence_score_neg, orga)
        
            
        
    ppi_poz, ppi_neg = delete_duplicates(ppi_poz, ppi_neg) 
    print(len(ppi_poz), len(ppi_neg))
    ppi_poz = list(set(ppi_poz))
    ppi_neg = list(set(ppi_neg))
    print(len(ppi_poz), len(ppi_neg))
     
    protein_go_anno = create_gene_GO_dict()
    
    ppi_poz, updated_protein_go_anno = filter_interactions(ppi_poz, protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)
    ppi_neg, updated_protein_go_anno = filter_interactions(ppi_neg, updated_protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)
    
    all_ppi = ppi_poz + ppi_neg
    labels =  [1] * len(ppi_poz) + [0] * len(ppi_neg)
    ##shuffle the data such that the poz and neg don't appear toghether
    full_dataset = list(zip(all_ppi, labels))
    random.shuffle(full_dataset)
    all_ppi, labels = zip(*full_dataset)
    full_dataset = Dataset_stringDB(all_ppi, labels, updated_protein_go_anno, go_id_dict_pth, go_embed_pth, shuffle, aliases_path, stringDB)

    sz = len(full_dataset)
    train, valid, test = data.random_split(full_dataset, [int(ratio[0]*sz), int(ratio[1]*sz) , sz - (int(ratio[0]*sz) + int(ratio[1]*sz)) ] )

    return train, valid, test, full_dataset



def get_dataset_split_stringDB2(confidence_score_pos, confidence_score_neg, go_id_dict_pth, go_embed_pth, shuffle, filter_sec, aliases_path = "", ratio = [0.8, 0.2, 0],\
                               stringDB = True, organism = [0], new_thingy = None, go_name_space_dict_pth = "datasets/transformerGO-dataset/go-terms/go_namespace_dict", go_filter = "ALL", intr_set_size_filter = [0,500], max_intr_size = None):
    
    """ Splitting up the interaction data into train/valid/test and generating embeddings 
    Args:
        confidence_score (float): the confidence score for the PPI
        go_id_dict_pth (str): path to the the GO id dict, e.g. GO1 -> 1
        shuffle (funct): function for shuffling the GO terms
        aliases_path (str): path to the aliases file 
        ratio (list): list of 3 floats specifing the split between train/valid/test
        stringDB (bool): weather we are using stringDB datasets or the Jains datasets
        go_name_space_dict_pth (str): path to the dictionary between name of GO terms and explanation e.g. GO1 -> 'MF molecular function'
        go_filter (str): filter for the GO terms (what terms to keep), e.g.  ALL, CC, BP, MF
        intr_set_size_filter (list): the range of the the GO set size: e.g. [0,10] meaning the range from 0 to 10
        max_intr_size (int): the maximum number of interactions to add to the dataset 
        
    Returns:
    Dataset_stringDB: 4 datasets objects for the train, valid, test and full datasets
    """ 
    ppi_poz = []
    ppi_neg = []
    
    if organism[0] == 0 and len(organism) == 1:
        ppi_poz = retrieve_interactions(1, confidence_score_pos)
        ppi_neg = retrieve_interactions(2, confidence_score_neg)
    else:
        for orga in organism:
            print('reached')
            ppi_poz += retrieve_organism_interactions(1, confidence_score_pos, orga)
            ppi_neg += retrieve_organism_interactions(2, confidence_score_neg, orga)
        
            
        
    ppi_poz, ppi_neg = delete_duplicates(ppi_poz, ppi_neg) 
    print(len(ppi_poz), len(ppi_neg))
    ppi_poz = list(set(ppi_poz))
    ppi_neg = list(set(ppi_neg))
    print(len(ppi_poz), len(ppi_neg))
     
    if new_thingy == None:
        protein_go_anno = create_gene_GO_dict()
        for key in protein_go_anno.keys():
            remove_go = []
            for c, go_term in enumerate(protein_go_anno[key]):
                if go_term not in frozenset(filter_sec.keys()):
                    remove_go.append(go_term)
            for removeja in remove_go:
                protein_go_anno[key].remove(removeja)
        with open('results/analysis/new_dict.pkl', 'wb') as f:
            pickle.dump(protein_go_anno, f)
    else:
        with open(new_thingy, 'rb') as f:
            protein_go_anno = pickle.load(f)
        
    
    ppi_poz, updated_protein_go_anno = filter_interactions(ppi_poz, protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)
    ppi_neg, updated_protein_go_anno = filter_interactions(ppi_neg, updated_protein_go_anno, go_id_dict_pth, aliases_path, stringDB, go_name_space_dict_pth, go_filter, intr_set_size_filter, max_intr_size)
    
    all_ppi = ppi_poz + ppi_neg
    labels =  [1] * len(ppi_poz) + [0] * len(ppi_neg)
    ##shuffle the data such that the poz and neg don't appear toghether
    full_dataset = list(zip(all_ppi, labels))
    random.shuffle(full_dataset)
    all_ppi, labels = zip(*full_dataset)
    full_dataset = Dataset_stringDB(all_ppi, labels, updated_protein_go_anno, go_id_dict_pth, go_embed_pth, shuffle, aliases_path, stringDB)

    sz = len(full_dataset)
    train, valid, test = data.random_split(full_dataset, [int(ratio[0]*sz), int(ratio[1]*sz) , sz - (int(ratio[0]*sz) + int(ratio[1]*sz)) ] )

    return train, valid, test, full_dataset



    
